<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Quay.io Documentation</title>

  <link href="https://coreos.com/dist/css/bootstrap.min.css" rel="stylesheet">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <link href="https://coreos.com/assets/css/docs.css" rel="stylesheet">
  <link href="https://coreos.com/assets/css/pygments-manni.css" rel="stylesheet">

  <link href="https://coreos.com/assets/css/coreos.css" rel="stylesheet">

  <link href="https://coreos.com/assets/css/syntax-highlight.css" rel="stylesheet">

  <link href="/stylesheets/docs.css" rel="stylesheet">

<!--[if lt IE 9]>
  <script src="https://coreos.com/assets/js/html5shiv.js"></script>
  <script src="https://coreos.com/assets/js/respond.min.js"></script>
  <![endif]-->

  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,400italic,600,700,900" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,500,600,700" rel="stylesheet" type="text/css">

  <link href="https://coreos.com/assets/css/gh-fork-ribbon.css" rel="stylesheet">
<!--[if IE]>
      <link rel="stylesheet" href="https://coreos.com/assets/css/gh-fork-ribbon.ie.css" />
<![endif]-->

  <script src="https://coreos.com/assets/js/jquery.js"></script>

  <style type="text/css">
  </style>
</head>

<body data-spy="scroll" data-target=".coreos-docs-sidebar">
  <div class="navbar navbar-fixed-top coreos-nav" role="navigation">
    <div class="container">
      <div class="navbar-header">
        <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".coreos-nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="/" target=""><img id="quay-logo" src="https://quay.io/static/img/quay-logo.png"></a>
      </div>
      <div class="navbar-collapse collapse coreos-nav-collapse">
        <ul class="nav navbar-nav">
         <li><a href="/">Community Documentation</a></li>
          <li><a href="/solution">Articles and Solutions</a></li>
          <li><a href="/guides">Guides</a></li>
          <li><a href="/glossary">Glossary</a></li>
          <li><a href="https://quay.io/contact">Support</a></li>
        </ul>
      </div>
    </div>
  </div>
  <div class="github-fork-ribbon-wrapper right fixed">
    <div class="github-fork-ribbon">
      <a href="https://github.com/coreos/quay-docs/blob/gh-pages/solution/repeatable-deployments.html">Fork me on GitHub</a>
    </div>
  </div>

  <div class="container">
    <div class="row">
      <div class="col-lg-9 col-md-9 col-sm-12">
        <div class="coreos-docs">
          <h1>Repeatable and Testable Deployments using Quay.io</h1>
          <h3><b>The perils of deployment</b></h3>

<p class="p3">Deploying to production: words that for many developers can chill their hearts to the core. Even with extensive testing and today’s scriptable deployment systems (Chef, Puppet, etc), deployment to production machines can still result in cryptic errors, odd behaviors and lots of inventive swearing. All this stress and headache stems from one primary issue: Production environments rarely, if ever, replicate staging or development environments completely. Unit tests are run on development machines or in continuous integration systems; integration tests are run on staging machines or by hand. These different environments try to replicate production as closely as possible, but even the slightest difference can result in disaster. Differences in libraries, file layouts, even minor revisions of the operating system, can result in unexpected behavior and downtime. We once had a production push breakage (in our previous product) as a result of <b>the</b> <b>method used to link a dependency</b>. Even though we had matched the versions of the library and the operating system exactly, we still encountered problems.</p>

<p class="p3">Fortunately, we discovered that Docker can come to the rescue, solving not only the environment problem, but helping to grant us further peace of mind by properly testing our deployments before they go to production.</p>

<p class="p3">This article will discuss how to setup Docker for <span class="s1">reproducible</span>, <span class="s1">tested</span>, deployments. If you need background on the terms used in this article, please read our article <a href="https://medium.com/devops-programming/7f5fd023158f"><span class="s2">What is Docker?</span></a> first.</p>

<h3><b>Being environmentally friendly</b></h3>

<p class="p3">The first major benefit of using Docker for deployment is its ability to reproduce a specific environment, quickly and on demand. A Docker container represents a sandbox of sorts, one in which a developer can finely control all dependencies of their software, including operating system version, library installation and supporting files. As a result, any software running inside the Docker container is guaranteed to be running under the same environment, whether on a developer’s machine, staging or production:</p>

<p class="p3">The reproducibility of the Docker container provides a level of comfort that current script-based deployment systems simply cannot match. A developer can develop a new version of a service on their machine, test it locally or on staging and then push the <b>same</b> <b>image</b> to production with the understanding that the testing environment <b>is</b> the production environment under which their code will now be running.</p>

<div class="article-image"><img src="image00.png"></div>

<h3><b>Preparing a Docker image</b></h3>
<p class="p3">Preparing images to run in Docker can be done in one of two ways: by manually building layers or via a Dockerfile, which allows for reproducible construction of images. Since reproducibility is the goal of our production deployments, we will write a Dockerfile that represents our production code and its dependencies. For this article, we’ll set up a simple web server that writes “Hello World”. The source code for the web server and its associated Dockerfile can be found here: <a href="https://github.com/DevTable/rtdexample"><span class="s2">https://github.com/DevTable/rtdexample</span></a></p>

<p class="p3">At its core, a Dockerfile is a simple list of commands building on top of a <b>base image</b>, with each command forming a new layer in the Docker image. The base image of a Docker image is extremely important: It defines the flavour of Linux used, and which tools are by available by default to the code running inside the container. Here at Quay.io, we recommend the <a href="https://github.com/phusion/baseimage-docker"><span class="s2">Phusion base image</span></a>, which runs a version of Ubuntu specially modified to run inside a Docker container.</p>

<p class="p3">To get started, we create a file named <b>Dockerfile</b> in the directory containing our existing code. The first line of a Dockerfile is, typically, the <b>FROM</b> command, which indicates the base image we are using (in this case, Phusion base). The Phusion base image can be found on the public Docker index at phusion/baseimage, so we place that directly in the FROM line:</p>

<pre class="dockerfile-example">FROM phusion/baseimage:0.9.9</pre>

<p class="p3">The <span class="s4">:0.9.9</span> above specifies that we are requesting that specific <b>tag</b> of the <span class="s4">baseimage</span> repository. Without a specific tag, the <span class="s4">latest</span> tag would always be used; any update to the <span class="s4">baseimage</span> repository could get pulled by our Dockerfile at any time, leading to possible breakages.</p>

<p class="p3">Once we have specified the base image, it is customary to add a MAINTAINER command. This “command” (which has no effects) indicates the author of the Dockerfile:</p>

<pre class="dockerfile-example">FROM phusion/baseimage:0.9.9
MAINTAINER John Smith &lt;<span class="s2">john.smith@example.com</span>&gt;</pre>

<h3><b>Working in a good ENVironment</b></h3>
<p class="p3">In order to use the Phusion base image we just setup, we need to tell it a home directory for our root user, as well as the fact that (under normal circumstances), the Docker image will not be running with an interactive terminal. Docker allows environment variables to be set using the <b>ENV</b> command, so we add two environment variables to our context;</p>

<pre class="dockerfile-example">ENV DEBIAN_FRONTEND noninteractive
ENV HOME /root</pre>

<h3><b>All your base are belong to this</b></h3>

<p class="p3">Now that we have the base image setup, our next step is to start adding the dependencies upon which our service will rely. Dockerfile’s provide a number of commands for adding dependencies; this article will cover the three or four most important commands needed for setting up the environment for our web service.</p>

<p class="p3">Our <a href="https://github.com/DevTable/rtdexample"><span class="s2">web service</span></a> is a Python application, which depends on <a href="https://pypi.python.org/pypi"><span class="s2">PyPI</span></a> to manage its own libraries. We therefore start by installing Python and PyPI:</p>

<pre class="dockerfile-example"># Install Ubuntu packages. New ubuntu packages should be added as their own apt-get install lines below the existing install commands.
RUN apt-get update
RUN apt-get install -y python-dev
RUN apt-get install -y python-pip</pre>


<p class="p3">The <b>RUN</b> command in a Dockerfile will run any command given, under the context and file system of the Docker container, and produce a new image layer with any resulting file system changes. We first run <span class="s4">apt-get update </span>to make sure our package manager is up to date. Then, in different calls, we ask our package manager to install (-y = without needing external confirmation) both python and PyPI.</p>

<p class="p3">As an aside: Why do we run each of the <span class="s4">apt-get -y install </span>commands on their own? The answer lies in how Docker handles <b>caching</b>. When building a Dockerfile, Docker will first check its own image cache to see if any of the layers previously built exist. If so, the build step will be skipped and the previous layer used. By placing each install command on its own, if we ever need to add or remove a package, we can do so without having to “reinstall” every other package used inside the Dockerfile, saving potentially large amounts of build time.</p>

<p class="p3">Once we’ve installed all the Ubuntu packages necessary to run our service, we conduct a bit of cleanup. This is not strictly necessary, but it does mean that the resulting Docker image is cleaner:</p>

<pre class="dockerfile-example"># Clean up any files used by apt-get
RUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*</pre>

<p class="p3">Our next step is to have PyPI install all the required libraries and packages necessary for our Python application. To do so, we’ll use the <span class="s4">pip install -r requirements.txt</span> convention, which installs all packages specified in a <span class="s4">requirements.txt</span> file.</p>

<p class="p3">The problem is…. how are we going to get our <span class="s4">requirements.txt </span>file into the proper directory? The answer: using the <b>ADD</b> command.</p>

<h3><b>ADDing dependencies</b></h3>
<p class="p3">The <b>ADD </b>command specifies that a file, located at the input path, should be copied from the same directory as the Dockerfile to the output path inside the container. For example, an <b>ADD</b> command like:</p>
<pre class="dockerfile-example">ADD config/somefile.txt myapp/anotherfile.txt</pre>

<p class="p3">will copy the file located at <span class="s4">config/somefile.txt </span>(relative to the Dockerfile’s directory) to <span class="s4">myapp/anotherfile.txt</span> relative to the working directory inside the container.</p>

<p class="p3">Therefore, for our purposes, it is simple to add our <span class="s4">requirements.txt</span> and install it. Note that we also call <b>WORKDIR</b> to ensure that all our commands are run inside the <span class="s4">/root</span> folder:</p>

<pre class="dockerfile-example"># Change our working directory to /root
WORKDIR /root

# Copy over our requirements.txt
ADD requirements.txt /root/requirements.txt

# Install our Python dependencies.
pip install -r requirements.txt</pre>

<h3><b>Asset management</b></h3>
<p class="p3">As we’ve seen, the <b>ADD</b> command can be used to add external files into the Docker image. Once we’ve installed our Python dependencies, our next step is to <b>ADD</b> any assets (including the web service’s code), to the Dockerfile. This can be done in steps or by copying entire directories:</p>

<pre class="dockerfile-example"># Copy over code and static assets.
ADD static /root/static
ADD application.py /root/application.py</pre>

<p class="p3">A question might be raised at this point: Why not copy all the static assets along with <span class="s4">requirements.txt </span>in the earlier part of the Dockerfile? The answer, once again, has to do with <b>caching</b>. The Dockerfile caching system will invalidate <b>all</b> steps after <b>any</b> <b>ADD</b> command that has a file that has changed, since it has been called. In our Dockerfile example, if we placed <span class="s4">ADD static /root/static</span> <b>before</b> the <span class="s4">pip install -r requirements.txt</span> call, then any time we changed a static asset file, the entire PyPI installation process would be called again. Since that is obviously not the behavior we’d like, you should try to <span class="s1">place dependencies as late as possible when ordering these commands</span>.</p>

<h3><b>Running your service</b></h3>

The final step in setting up a service via a Dockerfile is to have said Dockerfile invoke the service when the image is run. Since we are using the Phusion Base Image, we can take advantage of its embedded init service to run our application. To do so, we copy a shell script into the services directory. The shell script will be automatically run and started when the container first starts:</p>

<pre class="dockerfile-example"># Add my runservice.sh shell script as a service and make sure it has the proper flags
ADD runservice.sh /etc/service/mywebserver/run
RUN chmod +x /etc/service/mywebserver/run</pre>

Our final task is to tell Docker that we are exposing ports for our web service, and to call the init service described above. The <b>EXPOSE</b> command indicates to Docker the ports that should be exposed by the container when run and the <b>CMD</b> command tells Docker the command to run when the container is started:</p>

<pre class="dockerfile-example"># Tell Docker that we are exposing the HTTP ports
EXPOSE 443 80

# Finally, tell Docker to run the init service.
CMD ["/sbin/my_init"]</pre>

<p class="p3">We are now done our Dockerfile and can discuss different ways to build it.</p>

<h3><b>Building and deploying our new image</b></h3>

<p class="p3">Building a Dockerfile into an image is quite simple. In the directory that contains the Dockerfile and its data files (requirements.txt, static, etc), one simply executes:</p>

<pre class="command-example">docker build -t quay.io/mynamespace/myreponame .</pre>

<p class="p3">The command above tells Docker to <b>build</b> the Dockerfile and data found in the current directory and, if sucessful, to <b>tag</b> it as the repository named <span class="s4">quay.io/mynamespace/myreponame.</span></p>

<p class="note">Note: Don’t have a Quay.io username? <a href="https://quay.io/"><span class="s2">Sign up here</span></a>.</p>

<p class="p3">Once our build has been successful, we can then <b>push</b> our created image to Quay.io:</p>

<pre class="command-example">docker push quay.io/mynamespace/myreponame</pre>

<p class="p3">and on our production server, <b>pull </b>and<b> run </b>it:</p>

<pre class="command-example">docker run quay.io/mynamespace/myreponame -d</pre>

<p class="note">Note: The <span class="s4">-d</span> flag tells Docker to run the image in detached (background) mode, rather than waiting until the container exits to return the prompt.</p>

<h3><b>Tested deployments</b></h3>

<p class="p3">As we can see, it is a fairly simple and straightforward process to use a Dockerfile and <span class="s4">docker build</span> to create a reproducible runtime environment for our server, including all its dependencies, libraries and static assets. This is a very useful solution to a large portion of the deployment issues we discussed above, but it still leaves much to be desired; even if we have a test suite running on our code at all times, we still cannot be sure that our code runs the same under our test environment as our production environment.</p>

<p class="p3">Unit tests are the obvious solution; they can help to identify problems in our production image before we push it, and verify that it at least (partially) functions the way we intend. Since Docker allows us to run any command during the build process, we can take advantage of this fact to run our unit tests <b>while building the image</b>. If our tests succeed, we have some confidence that the runtime environment for our service is valid; if they fail, then the build fails, and we will have not created a potentially bad production image.</p>

<p class="p3">Running tests as part of the build process is quite easy; we simply execute a <b>RUN </b>command with our test runner. For example, since we are writing a Python web service, here is the testing command we’ll add to our Dockerfile:</p>

<pre class="dockerfile-example">RUN python -m unittest discover</pre>

<p class="p3">The above command will discover and run any unit tests found in our Python source code. To that end, we also need to make sure to <b>ADD</b> our unit tests to the assets inside our image:</p>

<pre class="dockerfile-example">ADD test /root/test</pre>

<p class="p3">Finally, we should delete any test files once our unit tests run. We do so to ensure that our production code cannot rely (accidentally or on purpose) on our test code:</p>

<pre class="dockerfile-example">RUN rm -rf test</pre>

<p class="p3">Altogether, this results in the following being added to the Dockerfile:</p>

<pre class="dockerfile-example"># Add my runservice.sh shell script as a service and make sure it has the proper flags
ADD runservice.sh /etc/service/mywebserver/run
RUN chmod +x /etc/service/mywebserver/run

# Test our production image.
ADD test /root/test
RUN python -m unittest discover
RUN rm -rf test

# Tell Docker that we are exposing the HTTP ports
EXPOSE 443 80

# Finally, tell Docker to run the init service.
CMD ["/sbin/my_init"]</pre>


<p class="p3">Every time we build our image, it’ll be fully tested as part of the build process, resulting in a <span class="s1">tested</span>, <span class="s1">reproducible</span> production environment!</p>

<h3><b>Automating this process</b></h3>

<p class="p3">While the above build process is very simple and straightforward, it suffers from one major downside: it requires a developer or devops engineer to rebuild the image before every production push. Recognizing this problem, Quay.io has <a href="http://blog.devtable.com/2014/03/link-your-quayio-repositories-to-github.html"><span class="s2">built support for automatically building Dockerfiles on pushes to any Github repository</span></a>. To setup, go to the Admin Panel for your Quay.io repository and create a new “Github repository trigger” under the “Build Triggers” tab. Once authenticated and setup, any time a developer pushes to your Github repository, Quay.io will pickup the changes, conduct the Dockerfile build and, if successful, push the new image to your repository. If the build fails for whatever reason (including test failures), results can be viewed in the build history.</p>

<p class="p3">As an example, we’ve created a repository in Quay.io that builds the Dockerfile and service contained in this article, which you can view here: <a href="https://quay.io/repository/quay/rtdeample"><span class="s2">https://quay.io/repository/quay/rtdeample</span></a></p>

<p class="p3">We think that Docker and Dockerfiles provide a unique ability for both developers and devops engineers to be more confident in their production deployment. When used with Quay.io’s automatic build systems, engineers can be confident that code is both tested and ready for deployment as soon as possible. <a href="https://quay.io/"><span class="s2">Try Quay.io today!</span></a></p>

        </div>
      </div>
      <div class="coreos-docs-sidebar col-lg-3 col-md-3 col-sm-12 hidden-xs">
        <ul class="nav coreos-docs-sidenav"></ul>
      </div>
    </div>
  </div>

  <script src="https://coreos.com/dist/js/bootstrap.min.js"></script>
  <script src="https://coreos.com/assets/js/holder.js"></script>
  <script src="https://coreos.com/assets/js/application.js"></script>
  <script src="https://coreos.com/assets/js/masonry.pkgd.min.js"></script>

  <script src="https://coreos.com/assets/js/jquery-ui-1.9.1.custom.min.js"></script>
  <script src="https://coreos.com/assets/js/jquery.tocify.js"></script>

  <script type="text/javascript">
  $(document).ready(function() {
    $('[data-toggle=tooltip]').tooltip();
  });
  </script>
  <script src="https://coreos.com/assets/js/docs.js"></script>
  <script type="text/javascript" src="https://coreos.com/assets/js/events.js"></script>
</body>
</html>
